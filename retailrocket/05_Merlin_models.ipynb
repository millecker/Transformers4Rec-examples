{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install conda if not available\n",
    "# curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o /tmp/miniconda-installer.sh\n",
    "# bash /tmp/miniconda-installer.sh\n",
    "\n",
    "# Setup conda environment\n",
    "# conda create -n rapids-22.02 -c rapidsai -c nvidia -c conda-forge python=3.8 cudatoolkit=11.2 cudf=22.02 dask-cudf=22.02\n",
    "# conda activate rapids-22.02\n",
    "# pip install nvtabular==0.11.0 tensorflow-gpu==2.8.0 merlin-models==0.2.0 transformers4rec==0.1.4 scipy==1.8.0 pynvml==11.4.1 ipykernel\n",
    "# python -m ipykernel install --user --name=rapids-22.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 16:40:07.219763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8080 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0001:00:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import cudf\n",
    "import dask_cudf\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import *\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin.schema import Schema\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "import merlin.models.tf.dataset as tf_dataloader\n",
    "\n",
    "from merlin.io.dataset import Dataset\n",
    "from merlin.schema.io.tensorflow_metadata import TensorflowMetadata\n",
    "from merlin.models.tf.blocks.core.aggregation import CosineSimilarity\n",
    "\n",
    "from utils.fit_transform import workflow_fit_transform\n",
    "from utils.save_visualize import save_results, print_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = './data/processed'\n",
    "train_path = './data/train/*.parquet'\n",
    "test_path = './data/test/*.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data folders\n",
    "!mkdir -p ./data/train ./data/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "TIME_KEY = \"timestamp\"\n",
    "USER_KEY = \"visitorid\"\n",
    "ITEM_KEY = \"itemid\"\n",
    "SESSION_KEY = \"sessionid\"\n",
    "TRANSACTION_KEY = \"transactionid\"\n",
    "TARGET_KEY = \"target\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to parquet\n",
    "train_sessions = pd.read_pickle(\"data/02_train_sessions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    260620\n",
       "1.0      8697\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Introduce target variable\n",
    "train_sessions[TARGET_KEY] = train_sessions[TRANSACTION_KEY].fillna(0)\n",
    "train_sessions.loc[train_sessions[TARGET_KEY] != 0, TARGET_KEY] = 1\n",
    "train_sessions[TARGET_KEY].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sessions.to_parquet(\"data/train/02_train_sessions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to parquet\n",
    "test_sessions = pd.read_pickle(\"data/02_test_sessions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    34715\n",
       "1.0     1308\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Introduce target variable\n",
    "test_sessions[TARGET_KEY] = test_sessions[TRANSACTION_KEY].fillna(0)\n",
    "test_sessions.loc[test_sessions[TARGET_KEY] != 0, TARGET_KEY] = 1\n",
    "test_sessions[TARGET_KEY].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sessions.to_parquet(\"data/test/02_test_sessions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>visitorid</th>\n",
       "      <th>event</th>\n",
       "      <th>itemid</th>\n",
       "      <th>transactionid</th>\n",
       "      <th>sessionid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1435607175</td>\n",
       "      <td>75</td>\n",
       "      <td>view</td>\n",
       "      <td>257575</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1435607242</td>\n",
       "      <td>75</td>\n",
       "      <td>view</td>\n",
       "      <td>257575</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1435609434</td>\n",
       "      <td>75</td>\n",
       "      <td>view</td>\n",
       "      <td>257575</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>99</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1435609596</td>\n",
       "      <td>75</td>\n",
       "      <td>view</td>\n",
       "      <td>257575</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>99</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1435609771</td>\n",
       "      <td>75</td>\n",
       "      <td>view</td>\n",
       "      <td>257575</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>99</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  visitorid event  itemid transactionid  sessionid  target\n",
       "0  1435607175         75  view  257575          <NA>         98     0.0\n",
       "1  1435607242         75  view  257575          <NA>         98     0.0\n",
       "2  1435609434         75  view  257575          <NA>         99     0.0\n",
       "3  1435609596         75  view  257575          <NA>         99     0.0\n",
       "4  1435609771         75  view  257575          <NA>         99     0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dask_cudf.read_parquet(train_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'visitorid', 'event', 'itemid', 'transactionid',\n",
       "       'sessionid', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visitorid</th>\n",
       "      <th>itemid</th>\n",
       "      <th>transactionid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>257575</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75</td>\n",
       "      <td>257575</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75</td>\n",
       "      <td>257575</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75</td>\n",
       "      <td>257575</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75</td>\n",
       "      <td>257575</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   visitorid  itemid transactionid  target\n",
       "0         75  257575          <NA>     0.0\n",
       "1         75  257575          <NA>     0.0\n",
       "2         75  257575          <NA>     0.0\n",
       "3         75  257575          <NA>     0.0\n",
       "4         75  257575          <NA>     0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[USER_KEY, ITEM_KEY, TRANSACTION_KEY, TARGET_KEY]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269317"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the total number or rows\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum item id: 15\n",
      "Maximum item id: 466864\n",
      "Unique item id: 39974\n"
     ]
    }
   ],
   "source": [
    "# Print item stats\n",
    "print('Minimum item id: ' + str(df[ITEM_KEY].min().compute()))\n",
    "print('Maximum item id: ' + str(df[ITEM_KEY].max().compute()))\n",
    "print('Unique item id: ' + str(df[ITEM_KEY].unique().shape[0].compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum user id: 75\n",
      "Maximum user id: 1407573\n",
      "Unique user id: 9474\n"
     ]
    }
   ],
   "source": [
    "# Print user stats\n",
    "print('Minimum user id: ' + str(df[USER_KEY].min().compute()))\n",
    "print('Maximum user id: ' + str(df[USER_KEY].max().compute()))\n",
    "print('Unique user id: ' + str(df[USER_KEY].unique().shape[0].compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    260620\n",
       "1.0      8697\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the distribution of the target class. We can see that the dataset is imbalanced.\n",
    "df[TARGET_KEY].value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merlin Models\n",
    "\n",
    "[Merlin Models](https://github.com/NVIDIA-Merlin/models) is a library to make it easy for users in industry or academia to train and deploy recommender models with best practices baked into the library. This will let users in industry easily train standard models against their own dataset, getting high performance GPU accelerated models into production. This will also let researchers to build custom models by incorporating standard components of deep learning recommender models, and then benchmark their new models on example offline datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 272 µs, sys: 95 µs, total: 367 µs\n",
      "Wall time: 373 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Define NVT the pipeline\n",
    "# We add tags for user_id, item_id and target. NVTabular will provide an output file\n",
    "# We categorify the user_id and item_id to be continuous integers 0, ..., |C|\n",
    "user_id = [USER_KEY] >> AddMetadata(tags=[Tags.USER_ID]) >> Categorify(freq_threshold=5)\n",
    "item_id = [ITEM_KEY] >> AddMetadata(tags=[Tags.ITEM_ID]) >> Categorify(freq_threshold=5)\n",
    "targets = [TARGET_KEY] >> AddMetadata(\n",
    "    tags=[str(Tags.BINARY_CLASSIFICATION), TARGET_KEY]\n",
    ")\n",
    "\n",
    "# Add more features\n",
    "add_feat = [\"event\"] >> nvt.ops.Categorify()\n",
    "\n",
    "# Add target encoding\n",
    "te_feat = (\n",
    "    [USER_KEY, ITEM_KEY] + add_feat >>\n",
    "    TargetEncoding(\n",
    "        [TARGET_KEY],\n",
    "        kfold=1,\n",
    "        p_smooth=20\n",
    "    ) >>\n",
    "    Normalize()\n",
    ")\n",
    "\n",
    "outputs = user_id + item_id + targets + add_feat + te_feat\n",
    "\n",
    "etl_description = 'etl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/dataframe.py:1253: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.33 s, sys: 183 ms, total: 3.51 s\n",
      "Wall time: 3.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Run NVTabular workflow\n",
    "workflow_fit_transform(outputs, train_path, test_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load schema\n",
    "schema = TensorflowMetadata.from_proto_text_file(output_path + '/train/').to_merlin_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = schema.select_by_tag(Tags.TARGET).column_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = mm.DCNModel(\n",
    "    schema,\n",
    "    depth=2,\n",
    "    deep_block=mm.MLPBlock([64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column, \n",
    "                                                 metrics=[tf.keras.metrics.AUC(), tf.keras.metrics.Accuracy(),\n",
    "                                                         tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    ")\n",
    "model_description = \"DCN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", run_eagerly=False)\n",
    "model_description = \"DCN_adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/dataframe.py:1253: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16 * 1024\n",
    "\n",
    "train_dl = tf_dataloader.BatchedDataset(\n",
    "    Dataset(output_path + '/train/*.parquet', part_size=\"500MB\"),\n",
    "    batch_size = batch_size,\n",
    "    label_names = [target_column],\n",
    "    shuffle= True,\n",
    "    schema = schema,\n",
    ")\n",
    "\n",
    "test_dl = tf_dataloader.BatchedDataset(\n",
    "    Dataset(output_path + '/test/*.parquet', part_size=\"500MB\"),\n",
    "    batch_size = batch_size,\n",
    "    label_names = [target_column],\n",
    "    shuffle = False,\n",
    "    schema = schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 16:40:14.522822: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/17 [=======================>......] - ETA: 0s - auc: 0.8317 - accuracy: 0.0000e+00 - precision: 0.1007 - recall: 0.6358 - loss: 0.5658 - regularization_loss: 0.0000e+00 - total_loss: 0.5658"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 16:40:20.387020: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: cond/then/_0/cond/cond/branch_executed/_122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 5s 95ms/step - auc: 0.8661 - accuracy: 0.0000e+00 - precision: 0.1203 - recall: 0.6634 - loss: 0.5163 - regularization_loss: 0.0000e+00 - total_loss: 0.5163 - val_auc: 0.9941 - val_accuracy: 0.0000e+00 - val_precision: 1.0000 - val_recall: 0.3846 - val_loss: 0.3124 - val_regularization_loss: 0.0000e+00 - val_total_loss: 0.3124\n",
      "CPU times: user 6.8 s, sys: 442 ms, total: 7.25 s\n",
      "Wall time: 6.95 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7c847ddfa0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(train_dl, validation_data=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc:0.8660605549812317\n",
      "accuracy:0.0\n",
      "precision:0.12029604613780975\n",
      "recall:0.6634471416473389\n",
      "loss:0.3338724672794342\n",
      "regularization_loss:0.0\n",
      "total_loss:0.3338724672794342\n",
      "val_auc:0.994050145149231\n",
      "val_accuracy:0.0\n",
      "val_precision:1.0\n",
      "val_recall:0.3845565617084503\n",
      "val_loss:0.3123938739299774\n",
      "val_regularization_loss:0.0\n",
      "val_total_loss:0.3123938739299774\n"
     ]
    }
   ],
   "source": [
    "# model_des = model_description + '_' + etl_description\n",
    "for key, value in  model.history.history.items():\n",
    "    print('%s:%s' % (key, value[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's delete the dataframe to free GPU-memory\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now shutdown the notebook to free GPU-memory\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
